{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def read_news_article(url):\n",
    "    \n",
    "    response = None\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        # Check if the request was successful (status code 200 means success).\n",
    "        if response.status_code == 200:\n",
    "            pass\n",
    "        else:\n",
    "            pass\n",
    "    except requests.Timeout:\n",
    "        return ''\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return ''\n",
    "    except requests.exceptions.JSONDecodeError as e:\n",
    "        return ''\n",
    "    except json.JSONDecodeError as e:\n",
    "        return ''\n",
    "    except requests.RequestException as e:\n",
    "        return ''\n",
    "\n",
    "    if (response is None):\n",
    "        return ''\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    article_tags = [\n",
    "        {'tag': 'div', 'attrs': {'class': 'article-content'}},\n",
    "        {'tag': 'div', 'attrs': {'class': 'caas-body'}},\n",
    "        {'tag': 'article'},\n",
    "        {'tag': 'section'},\n",
    "        {'tag': 'main'},\n",
    "        {'tag': 'p'},\n",
    "        {'tag': 'div', 'attrs': {'class': 'entry-content'}},\n",
    "        {'tag': 'div', 'attrs': {'class': 'content'}},\n",
    "        {'tag': 'div', 'attrs': {'class': 'body'}},\n",
    "        {'tag': 'div', 'attrs': {'id': 'content'}},\n",
    "        {'tag': 'div', 'attrs': {'class': 'article'}},\n",
    "        {'tag': 'section', 'attrs': {'class': 'article-body'}},\n",
    "        {'tag': 'section', 'attrs': {'class': 'content'}},\n",
    "        {'tag': 'section', 'attrs': {'class': 'entry'}},\n",
    "        {'tag': 'main', 'attrs': {'class': 'article-content'}},\n",
    "        {'tag': 'main', 'attrs': {'class': 'content'}},\n",
    "        {'tag': 'main', 'attrs': {'id': 'main-content'}},\n",
    "    ]\n",
    "\n",
    "    article_text = \"\"\n",
    "    for tag_info in article_tags:\n",
    "        tag = tag_info['tag']\n",
    "        attrs = tag_info.get('attrs', {})\n",
    "\n",
    "        elements = soup.find_all(tag, attrs=attrs)\n",
    "        for element in elements:\n",
    "            article_text += element.get_text() + \"\\n\"\n",
    "\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def is_grammatically_correct(sentence):\n",
    "    url = \"https://api.languagetool.org/v2/check\"\n",
    "\n",
    "    # Parameters for the API request\n",
    "    payload = {\n",
    "        \"text\": sentence,\n",
    "        \"language\": \"en-US\",\n",
    "    }\n",
    "\n",
    "    # Make the API request\n",
    "    response = requests.post(url, data=payload)\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except json.JSONDecodeError as e:\n",
    "        return False\n",
    "\n",
    "    # Check if any errors were found\n",
    "    if \"matches\" in data:\n",
    "        return len(data[\"matches\"]) == 0\n",
    "\n",
    "    return True  # Return True if no errors found or API response is unexpected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hello\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\hello\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def find_sentences_with_keyword(text, keywords):\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    sentences_with_keyword = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if keywords.lower() in sentence.lower():\n",
    "            sentence = sentence.replace(\"\\n\", \"\")\n",
    "            sentences_with_keyword.append(sentence)\n",
    "\n",
    "    sentences_processed = check_grammar(sentences_with_keyword)\n",
    "    return sentences_processed\n",
    "\n",
    "def check_grammar(sentences):\n",
    "    verified_sentences = []\n",
    "    for i in range (len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        if (is_grammatically_correct(sentence)):\n",
    "            verified_sentences.append(sentence)\n",
    "    return verified_sentences\n",
    "\n",
    "def perform_sentiment_analysis(sentence):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    sentiment_scores = sid.polarity_scores(sentence)\n",
    "    \n",
    "    return categorize_sentiment(sentiment_scores['compound'])\n",
    "\n",
    "def categorize_sentiment(sentiment_score):\n",
    "  if sentiment_score >= 0.75:\n",
    "    return 4\n",
    "  elif 0.75 > sentiment_score >= 0.5:\n",
    "    return 3\n",
    "  elif 0.5 > sentiment_score >= 0.25:\n",
    "    return 2\n",
    "  elif 0.25 > sentiment_score >= 0.05:\n",
    "    return 1\n",
    "  elif 0.05 > sentiment_score >= -0.05:\n",
    "    return 0\n",
    "  elif -0.05 > sentiment_score >= -0.25:\n",
    "    return -1\n",
    "  elif -0.25 > sentiment_score >= -0.5:\n",
    "    return -2\n",
    "  elif -0.5 > sentiment_score >= -0.75:\n",
    "    return -3\n",
    "  elif -0.75 > sentiment_score:\n",
    "    return -4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "def find_keywords_in_articles(articles, keywords):\n",
    "    sentiment_score = [0]*9\n",
    "    for article_url in articles:\n",
    "        article_content = read_news_article(article_url)\n",
    "        for keyword in keywords:\n",
    "            if re.search(r'\\b{}\\b'.format(keyword), article_content, re.IGNORECASE):\n",
    "                target_sentence = find_sentences_with_keyword(article_content, keyword)\n",
    "                if len(target_sentence) > 15:\n",
    "                    target_sentence = random.sample(target_sentence, 15)\n",
    "                for sentence in target_sentence:\n",
    "                    compound_score = perform_sentiment_analysis(sentence)\n",
    "                    sentiment_score[compound_score+4] += 1\n",
    "    return sentiment_score\n",
    "\n",
    "def mainfunction(urls, combined_terms, date):\n",
    "    combined_terms += ['Semiconductor']\n",
    "    sentiment_list = find_keywords_in_articles(urls, combined_terms)\n",
    "\n",
    "    appended_string = date\n",
    "    for sentiment in sentiment_list:\n",
    "        appended_string += \",\" + str(sentiment)\n",
    "    \n",
    "    append_file(\"newsdata.csv\", appended_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-08-22 2023-08-23\n"
     ]
    }
   ],
   "source": [
    "from pygooglenews import GoogleNews\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# default GoogleNews instance\n",
    "gn = GoogleNews(lang = 'en', country = 'US')\n",
    "\n",
    "startDatetime = datetime.strptime('2023-08-21', \"%Y-%m-%d\")\n",
    "endDatetime = datetime.strptime('2023-08-22', \"%Y-%m-%d\")\n",
    "startDate = startDatetime.strftime(\"%Y-%m-%d\")\n",
    "endDate = endDatetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "while (startDate != datetime.now().date().strftime(\"%Y-%m-%d\")):\n",
    "\n",
    "    combined_terms = [\"Applied Materials\"]\n",
    "    articleurls = []\n",
    "\n",
    "    articles_amat = gn.search('\"Applied Materials\"', from_ = startDate, to_ = endDate)\n",
    "    for i in range(len(articles_amat['entries'])):\n",
    "        articleurls.append(articles_amat['entries'][i]['link'])\n",
    "\n",
    "    articles_semi = gn.search('\"Semiconductor\"', from_ = startDate, to_ = endDate)\n",
    "    for i in range(len(articles_semi['entries'])):\n",
    "        articleurls.append(articles_semi['entries'][i]['link'])\n",
    "\n",
    "    if (len(articleurls) > 18):\n",
    "        articleurls = random.sample(articleurls, 18)\n",
    "\n",
    "    mainfunction(articleurls, combined_terms, startDate)\n",
    "\n",
    "    startDatetime = startDatetime + timedelta(days=1)\n",
    "    endDatetime = endDatetime + timedelta(days=1)\n",
    "    startDate = startDatetime.strftime(\"%Y-%m-%d\")\n",
    "    endDate = endDatetime.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    print(startDate, endDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append text to a file\n",
    "\n",
    "def append_file(name, text):\n",
    "  file = open(name, \"a\")  # Open file in append mode\n",
    "  file.writelines(''.join(text))  # Append content to the file\n",
    "  file.write(\"\\n\")\n",
    "  file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read text from a file\n",
    "\n",
    "def read_file(name):\n",
    "  file = open(name, \"r\")  # Open file in read mode\n",
    "  content = file.read()  # Read the entire content of the file\n",
    "  file.close()\n",
    "  return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new file\n",
    "\n",
    "def create_file(name):\n",
    "  file = open(name, \"w\")  # Open file in write mode\n",
    "  file.write(\"\")\n",
    "  file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfdml_plugin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
